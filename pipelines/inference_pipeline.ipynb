{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b87127d1-29d2-4ef7-bb22-be6e7cd58519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000303 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 75\n",
      "[LightGBM] [Info] Number of data points in the train set: 17593, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 16.700335\n",
      "Validation MAE: 5.534\n",
      "âœ” Saved feature pipeline â†’ /Users/kaushalshivaprakash/Desktop/project3/pipelines/models/feature_pipeline.pkl\n",
      "âœ” Saved model            â†’ /Users/kaushalshivaprakash/Desktop/project3/pipelines/models/best_model.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/kaushal/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# train_and_save.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from lightgbm import LGBMRegressor\n",
    "from joblib import dump\n",
    "\n",
    "# â”€â”€â”€ Helper functions at topâ€level so theyâ€™re pickleable â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def select_hour_bucket(df):\n",
    "    # Expects a DataFrame with a 'hour_bucket' column\n",
    "    return df[[\"hour_bucket\"]]\n",
    "\n",
    "def extract_datetime_parts(df):\n",
    "    # Expects a DataFrame with a 'hour_bucket' datetime column\n",
    "    return pd.DataFrame({\n",
    "        \"year\":  df[\"hour_bucket\"].dt.year,\n",
    "        \"month\": df[\"hour_bucket\"].dt.month,\n",
    "        \"day\":   df[\"hour_bucket\"].dt.day,\n",
    "        \"hour\":  df[\"hour_bucket\"].dt.hour\n",
    "    })\n",
    "\n",
    "def select_station_id(df):\n",
    "    # Expects a DataFrame with 'start_station_id'\n",
    "    return df[[\"start_station_id\"]]\n",
    "\n",
    "# â”€â”€â”€ 1) Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "PARQUET_PATH = \"/Users/kaushalshivaprakash/Desktop/project3/data/processed/cleaned_citibike/citibike_2023_top3.parquet\"\n",
    "MODEL_DIR    = \"/Users/kaushalshivaprakash/Desktop/project3/pipelines/models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# â”€â”€â”€ 2) Load raw rideâ€level data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "df = pd.read_parquet(PARQUET_PATH)\n",
    "df[\"started_at\"] = pd.to_datetime(df[\"started_at\"])\n",
    "df[\"hour_bucket\"] = df[\"started_at\"].dt.floor(\"H\")\n",
    "\n",
    "# â”€â”€â”€ 3) Aggregate to hourly counts per station â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "agg = (\n",
    "    df\n",
    "    .groupby([\"start_station_id\", \"hour_bucket\"])\n",
    "    .agg(target_trips=(\"ride_id\", \"count\"))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# â”€â”€â”€ 4) Prepare features X and target y â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "X = agg[[\"start_station_id\", \"hour_bucket\"]]\n",
    "y = agg[\"target_trips\"]\n",
    "\n",
    "# â”€â”€â”€ 5) Build preprocessing pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "datetime_pipeline = Pipeline([\n",
    "    (\"select_dt\", FunctionTransformer(select_hour_bucket, validate=False)),\n",
    "    (\"extract\",   FunctionTransformer(extract_datetime_parts, validate=False))\n",
    "])\n",
    "\n",
    "# For station ID, weâ€™ll wrap it in a named function as well\n",
    "station_pipeline = Pipeline([\n",
    "    (\"select_station\", FunctionTransformer(select_station_id, validate=False)),\n",
    "    (\"onehot\",         OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    (\"dt_feats\",   datetime_pipeline,    [\"hour_bucket\"]),\n",
    "    (\"loc_feats\",  station_pipeline,     [\"start_station_id\"])\n",
    "])\n",
    "\n",
    "# â”€â”€â”€ 6) Full modeling pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "pipeline = Pipeline([\n",
    "    (\"preproc\",   preprocessor),\n",
    "    (\"estimator\", LGBMRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# â”€â”€â”€ 7) Train/test split, fit, and validate â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "val_preds = pipeline.predict(X_val)\n",
    "mae = abs(val_preds - y_val).mean()\n",
    "print(f\"Validation MAE: {mae:.3f}\")\n",
    "\n",
    "# â”€â”€â”€ 8) Save artifacts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "feature_pipeline_path = os.path.join(MODEL_DIR, \"feature_pipeline.pkl\")\n",
    "model_path            = os.path.join(MODEL_DIR, \"best_model.pkl\")\n",
    "\n",
    "dump(preprocessor, feature_pipeline_path)\n",
    "print(f\"âœ” Saved feature pipeline â†’ {feature_pipeline_path}\")\n",
    "\n",
    "dump(pipeline.named_steps[\"estimator\"], model_path)\n",
    "print(f\"âœ” Saved model            â†’ {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9eb740a-edfe-49ad-8572-403ebd853900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/kaushal/lib/python3.11/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Predictions saved to /Users/kaushalshivaprakash/Desktop/project3/pipelines/output/predictions.csv\n",
      "ðŸš€ Inference pipeline created and executed successfully!\n"
     ]
    }
   ],
   "source": [
    "# inference_pipeline.py\n",
    "\"\"\"\n",
    "Standalone batch inference script reading Parquet input directly.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.pipeline import Pipeline as SklearnPipeline\n",
    "\n",
    "# â”€â”€ 1) CONFIGURE YOUR PATHS HERE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "FEATURE_PIPELINE = \"/Users/kaushalshivaprakash/Desktop/project3/pipelines/models/feature_pipeline.pkl\"\n",
    "MODEL_PATH       = \"/Users/kaushalshivaprakash/Desktop/project3/pipelines/models/best_model.pkl\"\n",
    "INPUT_PARQUET    = \"/Users/kaushalshivaprakash/Desktop/project3/data/processed/cleaned_citibike/citibike_2023_top3.parquet\"\n",
    "OUTPUT_CSV       = \"/Users/kaushalshivaprakash/Desktop/project3/pipelines/output/predictions.csv\"\n",
    "\n",
    "# â”€â”€ 2) LOAD & COMPOSE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def load_pipeline(pipeline_path: str, model_path: str) -> SklearnPipeline:\n",
    "    preprocessor = joblib.load(pipeline_path)\n",
    "    model        = joblib.load(model_path)\n",
    "    return SklearnPipeline([\n",
    "        (\"preprocessing\", preprocessor),\n",
    "        (\"estimator\",     model)\n",
    "    ])\n",
    "\n",
    "# â”€â”€ 3) BATCH INFERENCE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def run_batch_inference(pipeline: SklearnPipeline, input_path: str, output_path: str):\n",
    "    # Read Parquet input\n",
    "    df = pd.read_parquet(input_path)\n",
    "    # Ensure timestamp is datetime\n",
    "    df[\"started_at\"] = pd.to_datetime(df[\"started_at\"])\n",
    "    # Bucket by hour (must match training)\n",
    "    df[\"hour_bucket\"] = df[\"started_at\"].dt.floor(\"H\")\n",
    "    # Prepare feature frame\n",
    "    X = pd.DataFrame({\n",
    "        \"start_station_id\": df[\"start_station_id\"],\n",
    "        \"hour_bucket\":      df[\"hour_bucket\"]\n",
    "    })\n",
    "    # Run predictions\n",
    "    df[\"predicted_trips\"] = pipeline.predict(X)\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    # Write out full DataFrame with predictions\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Predictions saved to {output_path}\")\n",
    "\n",
    "# â”€â”€ 4) MAIN EXECUTION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def main():\n",
    "    # Validate existence of all paths\n",
    "    for path in (FEATURE_PIPELINE, MODEL_PATH, INPUT_PARQUET):\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"Required file not found: {path}\")\n",
    "    # Load the combined pipeline\n",
    "    pipeline = load_pipeline(FEATURE_PIPELINE, MODEL_PATH)\n",
    "    # Run and save predictions\n",
    "    run_batch_inference(pipeline, INPUT_PARQUET, OUTPUT_CSV)\n",
    "    # Final confirmation\n",
    "    print(\"ðŸš€ Inference pipeline created and executed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ee9a2f-8ed8-4e29-b515-8869e182a657",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
